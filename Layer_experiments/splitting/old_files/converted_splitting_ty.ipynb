{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper, TensorProto\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.util.basic import gen_finn_dt_tensor\n",
    "import os\n",
    "\n",
    "# Define a finder_fx\n",
    "def find_input_node(x):\n",
    "    return 'x' in x.input\n",
    "\n",
    "split_node = \"MultiThreshold_10\"\n",
    "# Open the tinyyolo model\n",
    "model = ModelWrapper(\"../tinyyolo_infershapes.onnx\")\n",
    "\n",
    "# Get onnx nodes\n",
    "nodes = model.graph.node\n",
    "passed_tensors,inialized_tensors=[],[]\n",
    "\n",
    "# Get all tensors in the model\n",
    "all_tensors = model.get_all_tensor_names()\n",
    "for ten in all_tensors:\n",
    "    # Make a list of all tensors which are initalized\n",
    "    if not model.get_initializer(ten) is None:\n",
    "        inialized_tensors.append(ten)\n",
    "        \n",
    "# Make a list of all nodes passed on the way to the split node\n",
    "for n in nodes:\n",
    "    if n.name != split_node:\n",
    "        passed_tensors.append(n.input)\n",
    "    else:\n",
    "        s_node = n        \n",
    "        break\n",
    "# Create a dict which has the passed initalizers\n",
    "init_tens = {}\n",
    "for pt in passed_tensors:\n",
    "    for t in pt:\n",
    "        if t in inialized_tensors:\n",
    "            init_tens[t] = model.get_initializer(t)\n",
    "start_node = s_node.input[0]\n",
    "\n",
    "# Find nodes upstream of the cut node\n",
    "upstream_nodes = model.find_upstream(start_node,find_input_node)\n",
    "\n",
    "# Reorder the nodes\n",
    "up_n_ordered = []\n",
    "for n in reversed(upstream_nodes):\n",
    "    up_n_ordered.append(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get input and output tensor shapes\n",
    "m_in = model.graph.input[0].name\n",
    "ish = model.get_tensor_shape(m_in)\n",
    "osh = model.get_tensor_shape(up_n_ordered[-1].output[0])\n",
    "\n",
    "# Make tensor value info for the input and output of the model\n",
    "inputs = helper.make_tensor_value_info(m_in,TensorProto.FLOAT,ish)\n",
    "outputs = helper.make_tensor_value_info(up_n_ordered[-1].output[0],TensorProto.FLOAT,osh)\n",
    "\n",
    "\n",
    "# Make a value info list to include in the graph\n",
    "value_info = []\n",
    "for t in init_tens.keys():\n",
    "    value_info.append(\n",
    "        model.get_tensor_valueinfo(t)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INT4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a graph\n",
    "new_graph = helper.make_graph(\n",
    "    name =\"new_graph\",\n",
    "    inputs=[inputs],\n",
    "    outputs=[outputs],\n",
    "    value_info=value_info,\n",
    "    nodes=up_n_ordered\n",
    ")\n",
    "\n",
    "# Create a new model that only contains the nodes desired\n",
    "split_model = ModelWrapper(helper.make_model(new_graph))\n",
    "# Set initalizer using the import model\n",
    "for t in init_tens.keys():\n",
    "    split_model.set_initializer(t,model.get_initializer(t))      \n",
    "    dt = model.get_tensor_datatype(t)\n",
    "    if  dt != \"FLOAT32\":\n",
    "        split_model.set_tensor_datatype(t,DataType[str(dt)])\n",
    "    \n",
    "split_model = split_model.transform(InferShapes())\n",
    "split_model = split_model.transform(InferDataTypes())\n",
    "if not os.path.exists(\"model_files\"):\n",
    "    os.mkdir(\"model_files\")\n",
    "else:\n",
    "    model_name = \"model_files/split_model_{}.onnx\".format(split_node)\n",
    "\n",
    "split_model.save(model_name)\n",
    "\n",
    "print(\"Split at node {}, and saved to {}\".format(split_node,model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0480a40de60fe0b4be044949303dbe98ce3610184aeb132e77bccbcbbb8df2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
