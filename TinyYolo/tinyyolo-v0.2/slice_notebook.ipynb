{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import TensorProto\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from slice_template import slice_node\n",
    "model = ModelWrapper(\"tinyyolo-20210831_v081.onnx\")\n",
    "# model.save(\"tinyyolo-20210831_v081.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************************************************************\n",
      "0\n",
      "{'input': ['673', None, None], 'starts': ['675', [1], array([0])], 'ends': ['676', [1], array([2])], 'axes': ['674', [1], array([4])], 'splits': ['677', [1], array([1])], 'output': ['678', None]}\n",
      "[1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ffe0027822ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                    dtype=TensorProto.INT64,node_name = s.name)\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Update dict is a dict that stores the new slice node with key = slice name, index and node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mupdate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnode_inx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;31m# store the model of the single slice node. This is for double checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shares/bulk/pgeel/FINNv0.8.1_repo/TY_build_KV260/finn/notebooks/MSC_Git_files/TinyYolo/tinyyolo-v0.2/slice_template.py\u001b[0m in \u001b[0;36mmake_node\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mMake\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mslice\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         '''\n\u001b[0;32m---> 43\u001b[0;31m         self.slice_node = helper.make_node(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;34m\"Slice\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/onnx/helper.py\u001b[0m in \u001b[0;36mmake_node\u001b[0;34m(op_type, inputs, outputs, name, doc_string, domain, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         node.attribute.extend(\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mmake_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/onnx/helper.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         node.attribute.extend(\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmake_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             if value is not None)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/onnx/helper.py\u001b[0m in \u001b[0;36mmake_attribute\u001b[0;34m(key, value, doc_string)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;31m# third, iterable cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mbyte_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_to_bytes_or_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;31m# Turn np.int32/64 into Python built-in int.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "from onnx import TensorProto\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from slice_template import slice_node\n",
    "\n",
    "# Open the Tinyyolo model\n",
    "tmp_model = ModelWrapper(\"tinyyolo-20210831.onnx\")\n",
    "\n",
    "# Create a dictionary to store the slice nodes in \n",
    "update_dict = {}\n",
    "# These are the inputs of a slice node in the correct order\n",
    "key_list = [\"input\", \"starts\", \"ends\", \"axes\", \"splits\", \"output\"]\n",
    "# These are the tensors from the replaced slice nodes\n",
    "drop_tensors = []\n",
    "\n",
    "# Loop over all the nodes in the graph at are of op_type Slice\n",
    "for s in tmp_model.get_nodes_by_op_type(\"Slice\"):\n",
    "    # Get the node index (maybe can remove?!)\n",
    "    node_inx = tmp_model.get_node_index(s)\n",
    "    # Store all tensor value info, initialization, and shapes for the current slice node\n",
    "    vinfo = [tmp_model.get_tensor_valueinfo(inp) for inp in s.input]\n",
    "    init = [tmp_model.get_initializer(inp) for inp in s.input]\n",
    "    shapes = [tmp_model.get_tensor_shape(i) for i in s.input]\n",
    "    # Get the input tensor names\n",
    "    t_names = [i for i in s.input]\n",
    "    \n",
    "    # Create a dict of attributes and initalize an empty dict [Tensor_name, shape, initialization value]\n",
    "    attr = {}\n",
    "    for k in key_list:\n",
    "        attr[k] = [None, None, None]\n",
    "    \n",
    "    # Loop over the len of shapes (not all slice nodes contain a splits value)\n",
    "    for ind in range(len(shapes)):\n",
    "        # Store the vlaue in attribute\n",
    "        attr[key_list[ind]] = [t_names[ind], shapes[ind], init[ind]]\n",
    "        # Append list of tensors for this node\n",
    "        drop_tensors.append(t_names[ind])\n",
    "\n",
    "    # Do the same as above but for the output of the slice node\n",
    "    for i in s.output:\n",
    "        attr[\"output\"] = [i, tmp_model.get_tensor_shape(i)]\n",
    "        drop_tensors.append(i)\n",
    "    # Print some stats\n",
    "    print(\"*\"*110)\n",
    "    print(attr[\"starts\"][2][0])\n",
    "    print(attr)\n",
    "    # Create a new slice node with attr instead of inputs\n",
    "    x = slice_node(input_shape=attr[\"input\"][1], output_shape=attr[\"output\"][1],\n",
    "                   starts_value=attr[\"starts\"][2], ends_value=attr[\"ends\"][2], axes_value=attr[\"axes\"][2], \n",
    "                   splits_value=attr[\"splits\"][2],input_tensor=attr[\"input\"][0], output_tensor=attr[\"output\"][0],\n",
    "                   dtype=TensorProto.INT64,node_name = s.name)\n",
    "    # Update dict is a dict that stores the new slice node with key = slice name, index and node\n",
    "    update_dict[s.name] = [node_inx, x.make_node()]\n",
    "    # store the model of the single slice node. This is for double checking\n",
    "    x.make_model(s.name+\".onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new model\n",
    "In this part of the notebook I will create a new model by removing the old slice nodes from the onnx file and replacing them with the new slice nodes created earlier in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "# Load in the tinyyolo model\n",
    "model = ModelWrapper(\"tinyyolo-20210831_v081.onnx\")\n",
    "\n",
    "# Create empty lists \n",
    "vinfo, keep_nodes, new_nodes = [], [], []\n",
    "# Create an empty dict for initialization tensors\n",
    "t_init = {}\n",
    "tensors = model.get_all_tensor_names()\n",
    "\n",
    "glb_in = model.graph.input[0]\n",
    "glb_out = model.graph.output[0]\n",
    "drop_tensors.append(glb_in.name)\n",
    "drop_tensors.append(glb_out.name)\n",
    "# Loop over all tenors in the model and remove the ones that belong to the dropped nodes\n",
    "for t in tensors:\n",
    "    # Store the value info and initialization values if they exist.\n",
    "    if not (t in drop_tensors):\n",
    "        vinfo.append(model.get_tensor_valueinfo(t))\n",
    "        if not model.get_initializer(t) is None:\n",
    "            t_init[t] = model.get_initializer(t)\n",
    "# These are the nodes that need to be removed\n",
    "replace_nodes = update_dict.keys()\n",
    "\n",
    "# Make a list of nodes to keep\n",
    "for n in model.graph.node:\n",
    "    if not n.name in replace_nodes:\n",
    "        keep_nodes.append(n)\n",
    "\n",
    "# Make a list of the new nodes\n",
    "new_nodes = [v[1] for k,v in update_dict.items()]\n",
    "for nnode in new_nodes:\n",
    "    keep_nodes.append(nnode)\n",
    "    \n",
    "model_config = {}\n",
    "model_config[\"opset_imports\"] = [onnx.helper.make_operatorsetid(\"\",11)]\n",
    "# Create a new model\n",
    "graph = onnx.helper.make_graph(\n",
    "        nodes= keep_nodes,\n",
    "        name=\"tinyyolo_infershapes_updated\",\n",
    "        inputs=[model.graph.input[0]],\n",
    "        outputs=[model.graph.output[0]],\n",
    "        value_info=vinfo[0:-2]\n",
    "        )\n",
    "new_model = ModelWrapper(onnx.helper.make_model(graph,**model_config))\n",
    "new_model.model.ir_version = 6\n",
    "print(new_model.model.ir_version)\n",
    "\n",
    "for k,v in t_init.items():\n",
    "    new_model.set_initializer(k,v)\n",
    "\n",
    "for t in tensors:\n",
    "    dt = model.get_tensor_datatype(t)\n",
    "    if not dt == \"FLOAT32\":\n",
    "        new_model.set_tensor_datatype(t,dt)\n",
    "        \n",
    "new_model.save(\"tinyyolo_slice_update.onnx\")\n",
    "infer_shapes = new_model.transform(InferShapes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from tinyyolo_slice_update.onnx\n",
      "Intermediate outputs will be generated in /workspace/results\n",
      "Final outputs will be generated in build-tinyyolo_slice_update\n",
      "Build log is at build-tinyyolo_slice_update/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/17]\n",
      "Running step: step_tidy_up [2/17]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/finn/src/finn/builder/build_dataflow.py\", line 166, in build_dataflow_cfg\n",
      "    model = transform_step(model, cfg)\n",
      "  File \"/workspace/finn/src/finn/builder/build_dataflow_steps.py\", line 243, in step_tidy_up\n",
      "    model = model.transform(FoldConstants())\n",
      "  File \"/workspace/qonnx/src/qonnx/core/modelwrapper.py\", line 140, in transform\n",
      "    (transformed_model, model_was_changed) = transformation.apply(transformed_model)\n",
      "  File \"/workspace/qonnx/src/qonnx/transformation/fold_constants.py\", line 59, in apply\n",
      "    oxe.execute_node(n, execution_context, graph)\n",
      "  File \"/workspace/qonnx/src/qonnx/core/onnx_exec.py\", line 74, in execute_node\n",
      "    output_list = sess.run(None, input_dict)\n",
      "  File \"/home/pgeel/.local/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 192, in run\n",
      "    return self._sess.run(output_names, input_feed, run_options)\n",
      "onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : At least one output should be requested.\n",
      "> /home/pgeel/.local/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py(192)run()\n",
      "-> return self._sess.run(output_names, input_feed, run_options)\n",
      "(Pdb) "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "! python build.py\n",
    "# version = {}\n",
    "# version[\"ir_version\"] = \"1.6.0\"\n",
    "# onnx.helper.make_model_gen_version(graph,**version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0480a40de60fe0b4be044949303dbe98ce3610184aeb132e77bccbcbbb8df2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
