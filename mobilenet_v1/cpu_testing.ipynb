{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU_testing/MultiThreshold_0_out0\n",
      "CPU_testing/MultiThreshold_1_out0\n",
      "CPU_testing/MultiThreshold_2_out0\n",
      "CPU_testing/MultiThreshold_3_out0\n",
      "CPU_testing/MultiThreshold_4_out0\n",
      "CPU_testing/MultiThreshold_5_out0\n",
      "CPU_testing/MultiThreshold_6_out0\n",
      "CPU_testing/MultiThreshold_7_out0\n",
      "CPU_testing/MultiThreshold_8_out0\n",
      "CPU_testing/MultiThreshold_9_out0\n",
      "CPU_testing/MultiThreshold_10_out0\n",
      "CPU_testing/MultiThreshold_11_out0\n",
      "CPU_testing/MultiThreshold_12_out0\n",
      "CPU_testing/MultiThreshold_13_out0\n",
      "CPU_testing/MultiThreshold_14_out0\n",
      "CPU_testing/MultiThreshold_15_out0\n",
      "CPU_testing/MultiThreshold_16_out0\n",
      "CPU_testing/MultiThreshold_17_out0\n",
      "CPU_testing/MultiThreshold_18_out0\n",
      "CPU_testing/MultiThreshold_19_out0\n",
      "CPU_testing/MultiThreshold_20_out0\n",
      "CPU_testing/MultiThreshold_21_out0\n",
      "CPU_testing/MultiThreshold_22_out0\n",
      "CPU_testing/MultiThreshold_23_out0\n",
      "CPU_testing/MultiThreshold_24_out0\n",
      "CPU_testing/MultiThreshold_25_out0\n",
      "CPU_testing/MultiThreshold_26_out0\n"
     ]
    }
   ],
   "source": [
    "from qonnx.util.basic import get_by_name\n",
    "from qonnx.transformation.create_generic_partitions import PartitionFromDict\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import os\n",
    "\n",
    "model = ModelWrapper(\"/home/pgeel/FINNv0.8.1_repo/build_KV260/finn/notebooks/MSC_Git_files/mobilenet_v1/models/mobilenet_streamline.onnx\")\n",
    "splits = []\n",
    "for n in model.graph.node:\n",
    "    if n.op_type == 'MultiThreshold':\n",
    "        splits.append(n.output[0])\n",
    "\n",
    "def custom_step_partition(stop_node):\n",
    "    output_dir = os.path.join(\"CPU_testing\",stop_node)\n",
    "    print(output_dir)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    upstream_0 = model.find_upstream(stop_node, lambda x: x.name == \"Conv_0\")\n",
    "    wanted_nodes = []\n",
    "    unwanted_nodes = []\n",
    "    for ind, node in enumerate(model.graph.node):\n",
    "        found_0 = get_by_name(upstream_0, node.name, \"name\") is not None\n",
    "        if found_0:\n",
    "            wanted_nodes.append(ind)\n",
    "        else:\n",
    "            unwanted_nodes.append(ind)\n",
    "    parent=model.transform(PartitionFromDict(\n",
    "        partitioning={ 0 : wanted_nodes, 1 : unwanted_nodes }, \n",
    "        partition_dir= output_dir\n",
    "    ))\n",
    "#     return ModelWrapper(output_dir+\"/partition_1.onnx\")\n",
    "\n",
    "for s in splits:\n",
    "    custom_step_partition(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{45: 209.05160903930664,\n",
       " 31: 138.76914978027344,\n",
       " 23: 100.30174255371094,\n",
       " 33: 154.87194061279297,\n",
       " 53: 290.3599739074707,\n",
       " 15: 50.423383712768555,\n",
       " 19: 72.51334190368652,\n",
       " 5: 1.667022705078125,\n",
       " 57: 352.4923324584961,\n",
       " 13: 34.73925590515137,\n",
       " 51: 284.4200134277344,\n",
       " 39: 185.4090690612793,\n",
       " 21: 83.70494842529297,\n",
       " 37: 156.16583824157715,\n",
       " 41: 190.5691623687744,\n",
       " 7: 18.42331886291504,\n",
       " 47: 236.69910430908203,\n",
       " 17: 58.408498764038086,\n",
       " 11: 32.36246109008789,\n",
       " 43: 206.96282386779785,\n",
       " 9: 24.120330810546875,\n",
       " 27: 121.88911437988281,\n",
       " 35: 153.36918830871582,\n",
       " 49: 260.1919174194336,\n",
       " 25: 101.85074806213379,\n",
       " 29: 124.22013282775879,\n",
       " 55: 341.86482429504395}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from onnxruntime_extensions import get_library_path, PyOp, onnx_op, PyOrtFunction\n",
    "import os\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "import numpy as np\n",
    "import time\n",
    "from onnx import helper\n",
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "import pandas as pd\n",
    "\n",
    "# CPU_TESTING\n",
    "rt = {}\n",
    "so = ort.SessionOptions()\n",
    "so.register_custom_ops_library(get_library_path())\n",
    "\n",
    "def set_multithreshold_default(model):#,save_model):\n",
    "    '''\n",
    "    Pass a modelproto model and the save file\n",
    "    '''\n",
    "    model = model.transform(DoubleToSingleFloat())\n",
    "    new_attr = [helper.make_attribute(\"out_scale\", 1.0),\n",
    "                helper.make_attribute(\"out_bias\", 0.0),\n",
    "                helper.make_attribute(\"data_layout\",\"NCHW\")]\n",
    "\n",
    "    for n in model.graph.node:\n",
    "        if n.op_type == \"MultiThreshold\":\n",
    "            out_scale,bias,datalayout = False,False,False\n",
    "            for na in n.attribute:\n",
    "                if na.name == \"out_scale\": out_scale = True\n",
    "                if na.name == \"out_bias\": bias = True\n",
    "                if na.name == \"data_layout\": datlayout = True\n",
    "            if not out_scale: n.attribute.append(new_attr[0])\n",
    "            if not bias: n.attribute.append(new_attr[1])\n",
    "            if not datalayout: n.attribute.append(new_attr[2])\n",
    "\n",
    "            n.domain = \"ai.onnx.contrib\"\n",
    "#     model.save(save_model)\n",
    "    return model\n",
    "\n",
    "def size_model(model):\n",
    "    cnt = 0\n",
    "    for n in model.graph.node:\n",
    "        cnt +=1\n",
    "    return cnt\n",
    "for j in os.listdir(\"CPU_testing\"):\n",
    "    if os.path.isdir(os.path.join(\"CPU_testing\",os.path.relpath(j))):\n",
    "        file = os.path.join(\"CPU_testing\",os.path.relpath(j),\"partition_1.onnx\")\n",
    "    #     Place onnx runtime code here, need to generate dummy data to pass to the onnx graph.\n",
    "        model = ModelWrapper(file)\n",
    "        model = set_multithreshold_default(model)\n",
    "\n",
    "        sess = ort.InferenceSession(model.model.SerializeToString(),so)\n",
    "        x = np.random.randint(0,255,sess.get_inputs()[0].shape).astype(np.float32)\n",
    "        start = time.time()\n",
    "        res = sess.run([],{sess.get_inputs()[0].name:x})\n",
    "        end = time.time()\n",
    "        rt[size_model(model)] = (end-start)*1e3\n",
    "rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"CPU_testing/runtimes.json\",'w') as f:\n",
    "    json.dump(rt,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
