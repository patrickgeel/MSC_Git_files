{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd89c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(1,\"../src/\")\n",
    "sys.path.insert(1,\"../src/finn/\")\n",
    "sys.path.insert(1,\"../src/qonnx/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddec9c0",
   "metadata": {},
   "source": [
    "# Create dummy inputs and store in the correct model directory \n",
    "For the moment want to create dummy input data, and keep this constant such that it does not change. Use this data to verify that the hw + cpu run has the same output as the expected output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06dc8f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  partition_0.onnx\tpartition_1.onnx\n",
      "data  partition_0.onnx\tpartition_1.onnx\n",
      "data  partition_0.onnx\tpartition_1.onnx\n",
      "data  partition_0.onnx\tpartition_1.onnx\n",
      "data  partition_0.onnx\tpartition_1.onnx\n",
      "data  partition_0.onnx\tpartition_1.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "clear_data = False\n",
    "\n",
    "# Create a directory with relative info\n",
    "model_dict = {}\n",
    "\n",
    "# Get the onnx model directory\n",
    "md = \"../models\"\n",
    "op_type = \"Conv\"\n",
    "\n",
    "# Get the model hw directory where the driver file is located.\n",
    "build_dir = \"../build-KV260/\"\n",
    "\n",
    "# Get split names for created hw files\n",
    "split_dirs = [s for s in os.listdir(os.path.join(build_dir,op_type))]\n",
    "\n",
    "# Fill the model dict\n",
    "for split_name in split_dirs:\n",
    "    hw_base_folder = os.path.join(build_dir,op_type,split_name,\"deploy\")\n",
    "    model_base_folder = os.path.join(md,op_type,split_name)\n",
    "    if clear_data:\n",
    "        shutil.rmtree(f\"{model_base_folder}/data\")\n",
    "    ! ls {model_base_folder}\n",
    "    model_dict[split_name.replace('_out0','')] = {\"model_dir\": model_base_folder, \"hw_dir\": hw_base_folder}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c181d",
   "metadata": {},
   "source": [
    "##### Eventually want to supply a image and see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2249e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data for the moment\n",
    "for k,v in model_dict.items():\n",
    "    sd = v[\"model_dir\"] + \"/data\"\n",
    "    if not os.path.exists(sd):\n",
    "        os.mkdir(sd)\n",
    "        x = np.random.random([1,3,224,224])\n",
    "        np.save(f\"{sd}/{k}_input.npy\",x)\n",
    "        v[\"data_dir\"] = sd\n",
    "    else:\n",
    "        v[\"data_dir\"] = sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a240b91",
   "metadata": {},
   "source": [
    "# Get golden output \n",
    "For each of the inputs get the expected output from the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 224, 224]\n",
      "[array([[644, 818, 530, 111, 626]], dtype=int64)]\n",
      "[array([[644, 818, 530, 111, 626]], dtype=int64)]\n",
      "[array([[644, 818, 530, 111, 626]], dtype=int64)]\n",
      "[array([[644, 818, 530, 111, 626]], dtype=int64)]\n",
      "[array([[644, 818, 530, 111, 626]], dtype=int64)]\n",
      "[array([[644, 818, 530, 111, 626]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from onnxruntime_extensions import get_library_path\n",
    "import onnxruntime as ort\n",
    "from custom_ort_functions import set_multithreshold_default\n",
    "# TODO: ADD THIS STEP TO THE CUSTOM_ORT_FUNCTIONS\n",
    "from qonnx.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "\n",
    "# Load input streamlined model\n",
    "mf = \"../models/mobilenet_streamline.onnx\"\n",
    "model = ModelWrapper(mf)\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "set_multithreshold_default(model,mf.replace('.onnx',\"_ort.onnx\"))\n",
    "\n",
    "# Set up a session for \n",
    "so = ort.SessionOptions()\n",
    "so.register_custom_ops_library(get_library_path())\n",
    "sess = ort.InferenceSession(mf.replace('.onnx',\"_ort.onnx\"), so)\n",
    "print(sess.get_inputs()[0].shape)\n",
    "\n",
    "for k,v in model_dict.items():\n",
    "    data_dir = v[\"data_dir\"]\n",
    "    x = np.load(f\"{data_dir}/{k}_input.npy\")\n",
    "    inp_dict = {sess.get_inputs()[0].name: x.astype(np.float32)}\n",
    "    _gld_res = sess.run([],inp_dict)\n",
    "    print(_gld_res)\n",
    "    np.save(f\"{data_dir}/{k}_gld_output.npy\",_gld_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2ca76",
   "metadata": {},
   "source": [
    "# Perform the hardware run for the dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e7f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "from driver_base import FINNExampleOverlay\n",
    "\n",
    "io_shape_dict = {\n",
    "    # FINN DataType for input and output tensors\n",
    "    \"idt\" : [DataType['UINT8']],\n",
    "    \"odt\" : [DataType['INT24']],\n",
    "    # shapes for input and output tensors (NHWC layout)\n",
    "    \"ishape_normal\" : [(1, 224, 224, 3)],\n",
    "    \"oshape_normal\" : [(1, 111, 111, 32)],\n",
    "    # folded / packed shapes below depend on idt/odt and input/output\n",
    "    # PE/SIMD parallelization settings -- these are calculated by the\n",
    "    # FINN compiler.\n",
    "    \"ishape_folded\" : [(1, 224, 224, 3, 1)],\n",
    "    \"oshape_folded\" : [(1, 111, 111, 2, 16)],\n",
    "    \"ishape_packed\" : [(1, 224, 224, 3, 1)],\n",
    "    \"oshape_packed\" : [(1, 111, 111, 2, 48)],\n",
    "    \"input_dma_name\" : ['idma0'],\n",
    "    \"output_dma_name\" : ['odma0'],\n",
    "    \"number_of_external_weights\": 0,\n",
    "    \"num_inputs\" : 1,\n",
    "    \"num_outputs\" : 1,\n",
    "}\n",
    "\n",
    "\n",
    "def exe_mode(batch_size = 1,\n",
    "    bitfile = \"../bitfile/finn-accel.bit\",\n",
    "    inputfile = \"input.npy\",\n",
    "    outputfile = \"output.npy\",\n",
    "    runtime_weight_dir = \"runtime_weights/\"\n",
    "            ):\n",
    "    \n",
    "        platform = \"zynq-iodma\"\n",
    "        accel = FINNExampleOverlay(\n",
    "        bitfile_name = bitfile, platform = platform,\n",
    "        io_shape_dict = io_shape_dict, batch_size = batch_size,\n",
    "        runtime_weight_dir = runtime_weight_dir\n",
    "        )\n",
    "     # load desired input .npy file(s)\n",
    "        ibuf_normal = []\n",
    "        for ifn in [inputfile]:\n",
    "            ibuf_normal.append(np.load(ifn))\n",
    "        obuf_normal = accel.execute(ibuf_normal)\n",
    "        if not isinstance(obuf_normal, list):\n",
    "            obuf_normal = [obuf_normal]\n",
    "        for o, obuf in enumerate(obuf_normal):\n",
    "            np.save(outputfile, obuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b24599aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Conv_6 --------------------\n",
      "../models/Conv/Conv_6_out0/data/Conv_6_hw_output.npy\n",
      "-------------------- Conv_10 --------------------\n",
      "../models/Conv/Conv_10_out0/data/Conv_10_hw_output.npy\n",
      "-------------------- Conv_0 --------------------\n",
      "../models/Conv/Conv_0_out0/data/Conv_0_hw_output.npy\n",
      "-------------------- Conv_2 --------------------\n",
      "../models/Conv/Conv_2_out0/data/Conv_2_hw_output.npy\n",
      "-------------------- Conv_4 --------------------\n",
      "../models/Conv/Conv_4_out0/data/Conv_4_hw_output.npy\n",
      "-------------------- Conv_8 --------------------\n",
      "../models/Conv/Conv_8_out0/data/Conv_8_hw_output.npy\n"
     ]
    }
   ],
   "source": [
    "exe = True\n",
    "for k,v in model_dict.items():\n",
    "    print(\"-\"*20,k,\"-\"*20)\n",
    "    hw_dir = v[\"hw_dir\"]   \n",
    "    data_dir = v[\"data_dir\"]\n",
    "    # Load and reshape the input\n",
    "    x = np.load(f\"{data_dir}/{k}_input.npy\")\n",
    "    x = x.reshape(io_shape_dict[\"ishape_normal\"][0])\n",
    "    x = x.astype(np.uint8)\n",
    "    # Save the reshaped data for hw input\n",
    "    np.save(f\"{data_dir}/{k}_hw_input.npy\",x)\n",
    "    # execute hw run \n",
    "    print(f\"{data_dir}/{k}_hw_output.npy\")\n",
    "    if os.path.exists(f\"{data_dir}\"):\n",
    "        for f in os.listdir(f\"{data_dir}\"):\n",
    "            if f\"{k}_hw_output.npy\" == f:\n",
    "                exe = False\n",
    "    if exe:\n",
    "        exe_mode(bitfile=f\"{hw_dir}/bitfile/finn-accel.bit\",\n",
    "                 inputfile=f\"{data_dir}/{k}_hw_input.npy\",\n",
    "                 outputfile=f\"{data_dir}/{k}_hw_output.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
