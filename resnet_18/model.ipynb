{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "from torch import nn, optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "import glob\n",
    "import random\n",
    "import logging\n",
    "#import genotypes\n",
    "import torch.utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "return_quant = True\n",
    "bit_width_input = 8\n",
    "bit_width_weight = 8\n",
    "bit_width_weight_b = 2\n",
    "bit_width_act = 2\n",
    "bit_width_pool = 8\n",
    "\n",
    "class RestNetBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(RestNetBasicBlock, self).__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False, weight_bit_width=bit_width_weight_b, return_quant_tensor=return_quant)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=bit_width_act, return_quant_tensor=return_quant)\n",
    "        self.conv2 = qnn.QuantConv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False, weight_bit_width=bit_width_weight_b, return_quant_tensor=return_quant)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=bit_width_act, return_quant_tensor=return_quant)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        \n",
    "        return self.relu2(self.relu2(x) + self.relu2(output))\n",
    "\n",
    "\n",
    "class RestNetDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(RestNetDownBlock, self).__init__()\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels, out_channels, kernel_size=3, stride=stride[0], padding=1, bias=False, weight_bit_width=bit_width_weight_b, return_quant_tensor=return_quant)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=bit_width_act, return_quant_tensor=return_quant)\n",
    "        self.conv2 = qnn.QuantConv2d(out_channels, out_channels, kernel_size=3, stride=stride[1], padding=1, bias=False, weight_bit_width=bit_width_weight_b, return_quant_tensor=return_quant)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=bit_width_act, return_quant_tensor=return_quant)\n",
    "        self.extra = nn.Sequential(\n",
    "            qnn.QuantConv2d(in_channels, out_channels, kernel_size=1, stride=stride[0], padding=0, bias=False, weight_bit_width=bit_width_weight, return_quant_tensor=return_quant),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        extra_x = self.extra(x)\n",
    "        extra_x = self.relu1(extra_x)\n",
    "        output = self.conv1(x)\n",
    "        out = self.bn1(output)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu1(out)\n",
    "        return self.relu2(extra_x + out)\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, criterion):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self._criterion = criterion\n",
    "        #self.quant_inp = qnn.QuantIdentity(bit_width=bit_width_input, return_quant_tensor=return_quant)\n",
    "        self.conv1 = qnn.QuantConv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False, weight_bit_width=bit_width_weight, return_quant_tensor=return_quant)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=bit_width_act, return_quant_tensor=return_quant)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = nn.Sequential(RestNetBasicBlock(64, 64, 1), RestNetBasicBlock(64, 64, 1))\n",
    "        self.layer2 = nn.Sequential(RestNetDownBlock(64, 128, [2, 1]), RestNetBasicBlock(128, 128, 1))\n",
    "        self.layer3 = nn.Sequential(RestNetDownBlock(128, 256, [2, 1]), RestNetBasicBlock(256, 256, 1))\n",
    "        self.layer4 = nn.Sequential(RestNetDownBlock(256, 512, [2, 1]), RestNetBasicBlock(512, 512, 1))\n",
    "        #self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.avgpool = qnn.QuantAvgPool2d(kernel_size=7, stride=1, bit_width=bit_width_pool)\n",
    "        self.fc = qnn.QuantLinear(512, 1000, bias=False, weight_bit_width=bit_width_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #out = self.quant_inp(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        #out = out.view(out.size(0), -1)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def _loss(self, input, target):\n",
    "        logits = self(input)\n",
    "        return self._criterion(logits, target)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
